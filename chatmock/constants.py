from __future__ import annotations

from typing import Any, Dict, List

# This file is generated by scripts/generate_constants.py
DEFAULT_CLIENT_ID = "app_EMoamEEZ73f0CkXaXp7hrann"
DEFAULT_OAUTH_ISSUER = "https://auth.openai.com"
DEFAULT_CHATGPT_RESPONSES_URL = "https://chatgpt.com/backend-api/codex/responses"

DEFAULT_OLLAMA_VERSION = "0.12.10"

OLLAMA_BASE_MODEL_IDS: List[str] = ["gpt-5", "gpt-5-codex", "codex-mini"]
OLLAMA_VARIANT_MODEL_IDS: List[str] = [
    "gpt-5-high",
    "gpt-5-medium",
    "gpt-5-low",
    "gpt-5-minimal",
    "gpt-5-codex-high",
    "gpt-5-codex-medium",
    "gpt-5-codex-low",
]

OLLAMA_MODEL_METADATA: Dict[str, Any] = {
    "modified_at": "2023-10-01T00:00:00Z",
    "size": 815_319_791,
    "digest": "8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc",
    "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": ["llama"],
        "parameter_size": "8.0B",
        "quantization_level": "Q4_0",
    },
}

OLLAMA_SHOW_RESPONSE: Dict[str, Any] = {
    "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /models/blobs/sha256:placeholder\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 100000\nPARAMETER stop \"</s>\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
    "parameters": "num_keep 24\nstop \"<|start_header_id|>\"\nstop \"<|end_header_id|>\"\nstop \"<|eot_id|>\"",
    "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
    "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": ["llama"],
        "parameter_size": "8.0B",
        "quantization_level": "Q4_0",
    },
    "model_info": {
        "general.architecture": "llama",
        "general.file_type": 2,
        "llama.context_length": 2000000,
    },
    "capabilities": ["completion", "vision", "tools", "thinking"],
}
